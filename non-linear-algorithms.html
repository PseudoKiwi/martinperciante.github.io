<!DOCTYPE HTML>
<!--
	Directive by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Directive by HTML5 UP</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Main -->
			<div id="main">

				<div class="box container">
					<header> 
						<h2>Non Linear Algorithms</h2>
					</header>

					<section>
						<header>
							<h3 style="text-align: left">Introduction</h3>
						</header>
						<p style="text-align: justify">
							Not always can we assume an input-output linear relation. What can we do in this situations? What algorithms exists?
							In this section, some algorithms will be studied, such as K-NN and Naive Bayes.
                        </p>
						<header>
							<h3 style="text-align: left">K-NN</h3>
						</header>
						<p style="text-align: justify">
							K-Nearest Neighbours is an algorithm based in distances calculated in an M-dimesion space where M is the amount of
							attributes available. To apply the model, a proximity function and a parameter K must be defined. The proximity function
							is used to measure the relationship between points in our mathematical space, and K is the number of closest neighbours
							needed to take a decision.
                        </p>
						<p style="text-align: justify">
							When a point must be classified, the K nearest examples of the training dataset are retrieved using the proximity function
							defined. Once we have the neighbours, the amount of each possible class between neighbours is computed. The class with most
							presence wins and that is the final prediction. When K=1, the label of the nearest point will be the prediction of the example.
						</p>
						<p style="text-align: justify">
							We established how the algorithm works, but proximity is yet to be defined. The first approach to this is the mathematical
							distance between points in the M-dimensional space. The problem here resides in attribute ranges and units. To improve
							predictions, data can be normalized to have a fair comparison between them. An extra improvement can be done. After identifying
							the K neighbours, weights can be assigned based on distance. By doing this, closer neighbours will have more importance
							and quantity wont be the only consideration.
						</p>
						<p style="text-align: justify">
							Other proximity concepts can be used. Correlation between points is the first alternative approach. When all attributes are
							binary, matching coefficients should be enough. This method counts how many of the total attributes are equal between both points.
							Finally, the cosine similarity computes the cosine between the vectors defined by the points.
						</p>
						<header>
							<h3 style="text-align: left">Naive Bayes</h3>
						</header>
						<p style="text-align: justify">
							This algorithm roots in probability and statistic methods. More specifically, in Bayes Theorem. It computes the probability of
							being from class Y having attributes x using the probability of x given class Y, being x the vector of independent attributes.
							Under this contition, probability can be expressed as:
						</p>
						<div style="text-align: center;"><img src="images/bayes.png"></div>
						<p>
							Class Y with the biggest P(Y|x) will be the prediction. Because P(x) is the same constant for every Y, this does not need to be
							calculated. P(Y) and each P(x|Y) can be obtained from the dataset. P(Y) is the fraction of Y examples in the dataset. Because of
							this, dataset size and amount of example must be representative of the population. P(x|Y) is analogous. For each attribute xi,
							we calculate the fraction of examples for each class.
						</p>
						<p>
							Knowing all probabilities, we just need to find the Y class which maximizes P(Y|x). Dividing the results by the sum of every
							possible P(Y|x), we get a percentage for each class. The Y with greatest percentage wins. When having continuous numeric values,
							probability densities may be used. An advantage of bayesian modeling is the missing value resistance. Problem arises when attributes
							do not provide examples for some categories. However, attribute should be independent for it to work correctly.
						</p>
						<header>
							<h3 style="text-align: left">Case Studies</h3>
						</header>
					</section>
					<h1></h1>
					<ul class="icons">
						<li><a href="index.html" class="fas fa-arrow-left"></a></li>
					</ul>
				</div>
			</div>
				
		<!-- Footer -->
		<div id="footer">
			<div class="container medium">
				<section>
					<header>
						<h2>Contact info</h2>
					</header>
					<hr />
					<section>
						<h4>Address</h4>
						<p>Dr. Joaqu√≠n Secco Illa 2827</p>
					</section>
					<hr />
					<section>
						<h4>Phone</h4>
						<p>(+598) 95 451 306</p>
					</section>
					<hr />
					<section>
						<h4>Email</h4>
						<p>tincho.perciante@gmail.com</p>
					</section>
				</section>
				<hr />
				
				<ul class="copyright">
					<li>&copy; Untitled. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
				</ul>
			</div>
		</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
