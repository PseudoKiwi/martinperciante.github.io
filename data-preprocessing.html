<!DOCTYPE HTML>
<!--
	Directive by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Directive by HTML5 UP</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Main -->
			<div id="main">

				<div class="box container">
					<header> 
						<h2>Data preprocessing</h2>
					</header>

					<section>
						<header>
							<h3 style="text-align: left">Introduction</h3>
						</header>
						<p style="text-align: justify">
							Raw data contains a lot of information, but not necessarily at plain sight. It is our job to treat the data
							correctly so we can squeeze out the most of it. However, many problems may arise. Some values may be missing,
							some datasets can have the same information but in a different scale, data distributions may not be ideal, and
							many more. This section is commited to talk about all the considerations needed before applying any kind of
							model to the data.
						</p>
						<header>
							<h3 style="text-align: left">Dataset Inspection</h3>
						</header>
						<p style="text-align: justify">
							First thing to identify are the input attributes and their data types. Input attributes are the starting point of
							our journey and we need to know what information we have. The target variable must be identified in this stage.
							Without knowing what we want to predict we can not do much. Data types include binomial, polynomial, real,
							text, and others. Not every algorithm works with every input. It is vital to know what we have to know when we 
							will be able to apply our model. Apropriate transformations can be used over data if needed.
						</p>
						<p style="text-align: justify">
							The amount of examples in the dataset is also a crucial aspect. Without the proper training, predictions made
							will not be significant. In some cases, small datasets are all we have and some techniques can be applyed to
							improve our predictions. For example, we can divide the dataset in k different sets. Then, we train k algorithms
							with k-1 datasets (each one of them lacking a different portion of the data) and the prediction will be the mean
							of the k results. We can also use sample with replacement instead of dividing into k different datasets. This
							techniques can also be used in Cross-Validation to estimate small dataset errors in predictions.
						</p>
						<p style="text-align: justify">
							In numeric type attributes, some statistical values may be useful. The mean value of the attribute gives us information
							about the most common value, and its standard deviation about how much we can expect a new value deviates from the mean.
							The range of values is also important. When working with data its crucial to know the start and end of the possible
							values so we can take better decisions.
						</p>
						<p style="text-align: justify">
							Another important aspect to take into account are data distributions. Not every algorithm works fine with every distribution.
							As I will present in future sections, some algorithms assume the data has a specific distribution. In this cases, mean and
							standard deviation are also part of the problem. We also need to be careful with the ammount of examples of each class
							(in classification problems) the training dataset has. An uneven distribution of examples will surely produce a bias error on
							the future predictions.
						</p>
						<p style="text-align: justify">
							Finally, outliers can be identified and treated too. An outlier is an example of the dataset that has abnormal values, or in
							a more mathematical way, its value is far away from the mean. Outliers can widely modify the statistical parameters and affect
							predictions. Depending on the problem, outliers must be removed. In some cases, identifying outliers is the algorithm purpose,
							but in general they will cause more damage than good.
						</p>
						<header>
							<h3 style="text-align: left">Feature Selection</h3>
						</header>
						<p style="text-align: justify">
							After understanding the problem, decisions must be taken to get the better results out of the dataset. Not every attribute gives
							us useful information to solve our problem. attributes that have no relation with the output variable should be removed from the
							training. We need to be cautious in this step. To remove an attribute we first have to study its relevance. If not careful enough,
							we can be mislead by our intuition to remove data that was actually relevant to our predictions.
						</p>
						<p style="text-align: justify">
							Correlation between attributes is very important. The correlation between variables is a measurment of how much one of them change
							when the other does. If an input has near 0 correlation with the output variable, it can be safely removed for the training. However,
							high correlation between inputs should be avoided. If two input are highly correlated they are giving essentially the same information,
							causing overfitting in the model. The correlation matrix is an useful tool to see all this information at one place. It is not the objective
							here to enter in Feature Selection techniques, but to state its importance before modeling.
						</p>
						<header>
							<h3 style="text-align: left">Missing Values</h3>
						</header>
						<p style="text-align: justify">
							Once we filter the attributes, other considerations must be taken. What can we do when data values are missing? Depends on what we need
							and how much information we have. Dealing with missing values is necessary because many algorithms will crash if the dataset is incomplete.
						</p>
						<p style="text-align: justify">
							The first solution we can arrive to is deleting the examples with missing attributes. When the amount of examples with missing values of some
							category is small compared with the total amount of examples, we can safely erradicate them from the dataset. Being a small amount of examples
							imply that they alone can not modify significantly the statistical parameters, so this is a plausible option. If most of one category values
							are missing, maybe the best solution is to delete the hole input column because not much information is given about it, so no conclusions can
							be taken.
						</p>
						<p style="text-align: justify">
							Instead od deleting, we could also imputate some values. Different imputation techniques are used. The mean, maximum or minimum values are
							usually the ones imputed in the datasets. We should be careful with imputation because we may add a non negligible amount of error in our predictions.
						</p>
						<header>
							<h3 style="text-align: left">Normalization</h3>
						</header>
						<p style="text-align: justify">
							Sometimes we will need inputs to be in the same range of values. Normalization is the process to transform the actual range of values into
							0 to 1 values. The disadvantage here is that we lose information about the actual values and can only know how near the max or min value our
							data is. This can also have problems when applying the model, because we need this data to be in the same initial range of values. However, it
							is a useful tool. The transformation to get the data in this new range is:
						</p>
						<div style="text-align: center;"><img src="images/normalizacion.png" width="40%"></div>
						<p style="text-align: justify">
							Minimum values will then be 0 and maximum values will be 1.
						</p>
						<header>
							<h3 style="text-align: left">Standarization</h3>
						</header>
						<p style="text-align: justify">
							Standarization is another data transformation. In this case, it produces a 0 mean, 1 standard deviation data distribution. Ideally, this will
							be enough to transform our data into a gaussian like distribution. However, for this to happen we need a large ammount of thata. Other transformations
							can be used to achieve this, such as a logarithm transform. The reason for this is because some algorithms assume a normal distribution in data inputs.
							If we can get the data to be gaussian-like, we will be able to use them. The transformation is like follows:
						</p>
						<div style="text-align: center;"><img src="images/estandarizacion.png" width="40%"></div>
						<p style="text-align: justify">
							This transformation is also known as the z-transformation
						</p>
						<header>
							<h3 style="text-align: left">Dataset compatibility with model</h3>
						</header>
						<p style="text-align: justify">
							There is a reason why we train algorithms, we want to use them. However, not every dataset may be compatible with the trained model. Every 
							data consideration mentioned before must be taken again with the new dataset. attribute ranges and types must be the same. Data distribution
							should also be at least similar to the training one.
						</p>
						<p style="text-align: justify">
							Another important aspect of the data is its context. An algorithm trained for predicting physical strength in american athletes may not be
							useful with asian athletes. If every data aspect is correct, the model will give us the predictions, but this will not necessarily be accurate.
							This should rise awareness of the importance of datasets. If mathematically compatible, any model will give its prediction. However, trash data
							will give trash results. Understanding the boundries of our solutions will certainly help us discern where its applicable and where not.
						</p>
						<header>
							<h3 style="text-align: left">Conclusion</h3>
						</header>
						<p style="text-align: justify">
							Data preprocessing may be the most tedious partof the Machine Learning experience, but also the most important. Without a proper treatment and
							understanding of the problem, we wont be able to get the best of it. With deep understanding of the situation, decisions like which algorithm
							to use become trivial. Like gem stones, data must be polished before using it, so it can shine and show everyone its beuty.
						</p>
					</section>
					<h1></h1>
					<ul class="icons">
						<li><a href="index.html" class="fas fa-arrow-left"></a></li>
					</ul>
					
				</div>
			</div>
				
		<!-- Footer -->
		<div id="footer">
			<div class="container medium">
				<section>
					<header>
						<h2>Contact info</h2>
					</header>
					<hr />
					<section>
						<h4>Address</h4>
						<p>Dr. Joaquín Secco Illa 2827</p>
					</section>
					<hr />
					<section>
						<h4>Phone</h4>
						<p>(+598) 95 451 306</p>
					</section>
					<hr />
					<section>
						<h4>Email</h4>
						<p>tincho.perciante@gmail.com</p>
					</section>
				</section>
				<hr />
				
				<ul class="copyright">
					<li>&copy; Untitled. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
				</ul>
			</div>
		</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
